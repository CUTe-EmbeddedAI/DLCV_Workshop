{"cells":[{"cell_type":"markdown","metadata":{"id":"Xq0pqNs7HBsY"},"source":["## Skip Connections\n","\n","Learn about skip connections and the problem they solve.\n","\n","We will be covering:\n","    \n","- The update rule and the vanishing gradient problem\n","\n","\n","- Skip connections for the win\n","\n","\n","- ResNet: skip connections via addition\n","\n","\n","- DenseNet: skip connections via concatenation\n","\n","If you were trying to train a neural network back in 2014, you would have definitely observed the so-called **vanishing gradient problem**. In simple terms: you are behind the screen checking the training process of your network, and all you see is that the training loss stops decreasing but is still far away from the desired value. You check all your code lines to see if something was wrong all night and you find no clue.\n","\n","### The update rule and the vanishing gradient problem\n","\n","Let’s revisit ourselves the update rule of gradient descent without momentum, given L to be the loss function and $\\lambda$ to be the learning rate:\n","\n","> $w_{i}' = w_{i} + \\Delta w_{i}$\n","\n","where $\\Delta w_{i} = - \\lambda \\frac{\\delta C}{\\delta \\Delta w_{i}}$\n","\n","You try to update the parameters by changing them with a small amount $\\Delta w_{i}$ that was calculated based on the gradient. For instance, let’s suppose that for an early layer the average gradient is 1e-15 $(\\Delta L/ \\delta w)$. Given a learning rate of 1e-4 ( $\\lambda$ in the equation), you basically change the layer parameters by the product of the referenced quantities, which is 1e-19 ($\\Delta w_{i}$). As a result, **you don’t actually observe any change in the model while training your network**. This is how you can observe the vanishing gradient problem.\n","\n","### Skip connections for the win\n","\n","At present, skip connection is a standard module in many CNN architectures. By using a skip connection, we provide an alternative path for the gradient.\n","\n","It is experimentally validated that these additional paths are often beneficial for the convergence of the model.\n","\n","> Skip connections, as the name suggests, **skip some layer in the neural network and feed the output of one layer as the input to the next layers**, instead of just the next one.\n","\n","As explained in the previous chapter, using the chain rule, we keep multiplying terms with the error gradient as we go backwards. However, in the long chain of multiplication, if we multiply many things together that are less than one, the resulting gradient will be very small.\n","\n","Thus, **the gradient becomes very small** as we approach the earlier layers in a deep network. In some cases, the gradient becomes zero, meaning that **we do not update the early layers at all**.\n","\n","In general, there are two fundamental ways that one could use skip connections through different non-sequential layers:\n","\n","a) **Addition**, as in residual architectures\n","\n","b) **Concatenation**, as in densely connected architectures"]},{"cell_type":"markdown","metadata":{"id":"3b5tg7p2HBsx"},"source":["### ResNet: skip connections via addition\n","\n","The core idea is to backpropagate through the identity function by just using vector addition. The gradient would then simply be multiplied by one and its value will be maintained in the earlier layers. This is the main idea behind Residual Networks (ResNets): they stack these skip residual blocks together. We use an identity function to **preserve the gradient**.\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig28.PNG)\n","\n","Mathematically, we can represent the residual block and calculate its partial derivative (gradient), given the loss function $L$ like this:\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig29.PNG)\n","\n","Apart from the vanishing gradients, there is another reason for commonly using skip connections. For a plethora of tasks (such as semantic segmentation, optical flow estimation, etc.), some information was captured in the initial layers. We would like to allow the later layers to also learn from them.\n","\n","It has been observed that in **earlier layers, the learned features correspond to lower semantic information** that is extracted from the input. If we had not used the skip connection, that information would have turned too abstract.\n","\n","Ready for a coding exercise? Let’s try to implement a skip connection in Pytorch. In the code below, you will find a small network of two convolutional layers. Your goal is to write the `forward` function so that the input is added to the output of the two layers, forming a residual connection. In essence, you will represent the exact above image in code."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Rt-rjtLUHBs7","executionInfo":{"status":"ok","timestamp":1658110955719,"user_tz":-480,"elapsed":7,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","seed = 172\n","torch.manual_seed(seed)\n","\n","\n","class SkipConnection(nn.Module):\n","\n","    def __init__(self):\n","        super(SkipConnection, self).__init__()\n","        self.conv_layer1 = nn.Conv2d(3, 6, 2, stride=2, padding=2)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv_layer2 = nn.Conv2d(6, 3, 2, stride=2, padding=2)\n","        self.relu2 = nn.ReLU(inplace=True)\n","\n","    def forward(self, input: torch.FloatTensor) -> torch.FloatTensor:\n","        # WRITE YOUR CODE HERE\n","        pass"]},{"cell_type":"markdown","metadata":{"id":"VchLM_HgHBtE"},"source":["### Solution"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"x8GvgqfPHBtH","executionInfo":{"status":"ok","timestamp":1658110969957,"user_tz":-480,"elapsed":325,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","seed = 172\n","torch.manual_seed(seed)\n","\n","\n","class SkipConnection(nn.Module):\n","\n","    def __init__(self):\n","        super(SkipConnection, self).__init__()\n","        self.conv_layer1 = nn.Conv2d(3, 6, 2, stride=2, padding=2)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv_layer2 = nn.Conv2d(6, 3, 2, stride=2, padding=2)\n","        self.relu2 = nn.ReLU(inplace=True)\n","\n","    def forward(self, input: torch.FloatTensor) -> torch.FloatTensor:\n","        x = self.conv_layer1(input)\n","        x = self.relu(x)\n","        x = self.conv_layer2(x)\n","        x = self.relu2(x)\n","        return x + input"]},{"cell_type":"markdown","metadata":{"id":"DsUO_agJHBtJ"},"source":["## DenseNet: skip connections via concatenation\n","\n","For some prediction problems, **there is low-level information shared between the input and output, and it would be desirable to pass this information directly across the net**. The alternate way to achieve skip connections is by concatenation of previous feature maps. The most famous deep learning architecture is DenseNet.\n","\n","This architecture heavily uses feature concatenation so as to ensure maximum information flow between layers in the network. This is achieved by **connecting all layers directly with each other via concatenation**, as opposed to ResNets. Practically, what you do is concatenate the feature channel dimension. This leads to:\n","\n","a) an **enormous amount of feature channels on the last layers** of the network;\n","\n","b) more **compact** models;\n","\n","c) extreme **feature reusability**.\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig30.PNG)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"11. Skip_connections.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}