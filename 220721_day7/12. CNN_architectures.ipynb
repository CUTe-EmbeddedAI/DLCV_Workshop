{"cells":[{"cell_type":"markdown","metadata":{"id":"4Unevjg3Hj5p"},"source":["## CNN Architectures\n","\n","Explore the most innovative CNN architectures (available from 2012 until 2021) and their principles.\n","\n","We will be covering the following:\n","\n","- Alexnet\n","\n","- VGGNet\n","\n","- InceptionNet/ GoogleNet\n","\n","In this section, we will discuss CNN architectures that stood the test of time. Even though not all of them are still used in recent top-performing architectures, it is important to study them and understand their intuitions.\n","\n","### AlexNet\n","\n","AlexNet is made up of 5 conv layers starting from an 11x11 kernel. It was the first architecture that employed max-pooling layers, Relu activation functions, and dropout for the 3 enormous linear layers. The network was used for image classification with 1000 possible classes, which for that time was madness (it was introduced in 2012). Now you can implement it in 35 lines of Pytorch code.\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig31.png)\n","\n","> It was the first convolutional model that was successfully trained on Imagenet, a dataset with 1M training images of 1000 classes."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"530tnWNvHj50","executionInfo":{"status":"ok","timestamp":1658111067635,"user_tz":-480,"elapsed":2854,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}},"outputId":"774286ff-cffa-41f6-be4a-07812915feec"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 10])\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class AlexNet(nn.Module):\n","    def __init__(self, num_classes: int = 1000) -> None:\n","        super(AlexNet, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","        )\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(),\n","            nn.Linear(256 * 6 * 6, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(4096, num_classes),\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x\n","\n","model = AlexNet(num_classes=10)\n","inp = torch.rand(1,3,128,128)\n","print(model(inp).shape)"]},{"cell_type":"markdown","metadata":{"id":"CTAYJpAaHj54"},"source":["### VGG\n","\n","The famous paper “Very Deep Convolutional Networks for Large-Scale Image Recognition” made the term “deep” viral. It was the first study that provided undeniable evidence that simply adding more layers increases performance. Nonetheless, this assumption holds true up to a certain point. The authors used only 3x3 kernels, as opposed to AlexNet. The architecture was trained using 224 × 224 RGB images.\n","\n","The main principle is that a stack of three $3 \\times 3$ conv layers are similar to a single $7 \\times 7$ layer. And maybe even better because they use three non-linear activations in between (instead of one), which makes the function more discriminative.\n","\n","Secondly, this design decreases the number of parameters. Specifically, you need $3*(3^2)C^2 = 27 \\times C^2$ weights, compared to a $7 \\times 7$ conv. layer that would require $1*(7^2)C^2 = 49C^2$ parameters (81% more).\n","\n","Intuitively, it can be regarded as a regularisation on the $7 \\times 7$ conv. filters, constricting them to have a $3 \\times 3$ non-linear decomposition.\n","\n","Finally, to get a visual comparison next to AlexNet:\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig32.PNG)"]},{"cell_type":"markdown","metadata":{"id":"aUpfQejtHj56"},"source":["### InceptionNet/GoogleNet\n","\n","After VGG, the paper “Going Deeper with Convolutions” was a huge breakthrough.\n","\n","> Motivation: Increasing the depth (number of layers) is not the only way to make a model bigger. What about increasing both the depth and width of the network while keeping computations to a constant level?\n","\n","This time the inspiration comes from the human visual system, wherein information is processed at multiple scales and then aggregated locally. How can this be achieved without a memory explosion?\n","\n","With $1 \\times 1$ convolutions! The main purpose is dimension reduction by reducing the output channels of each convolution block. Then we can process the input with different kernel sizes. As long as the output is padded, it is the same as in the input.\n","\n","To find the appropriate padding with single stride convs without dilation, padding $p$ and kernel $k$ are defined so that $out=in$ (input and output spatial dims):\n","\n","$out = in + 2*p - k + 1$, which means that $p= (k-1)/2$. This way, we can concatenate features convolved with different kernels.\n","\n","Then, we need the $1 \\times 1$ convolutional layer to “project” the features to fewer channels in order to win computational power. With these extra resources, we can add more layers. Actually, the $1 \\times 1$ convolutions work similar to a low dimensional embedding.\n","\n","This in turn allows to not only increase the depth, but also the width of the famous GoogleNet by using Inception modules. The core building block, called the inception module, looks like this:\n","\n","![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/fig33.PNG)\n","\n","The whole architecture is called GoogleNet or InceptionNet. In essence, the authors claim that they try to approximate a sparse convnet with normal dense layers (as shown in the figure).\n","\n","Why? Because they believe that only a small number of neurons are effective.\n","\n","Moreover, **it uses convolutions of different kernel sizes ($5 \\times 5$, $3 \\times 3$, $1 \\times 1$) to capture details at multiple scales**.\n","\n","> In general, a larger kernel is preferred for information that resides globally, and a smaller kernel is preferred for information that is distributed locally.\n","\n","Besides, **1 \\times 1** convolutions are used to compute reductions before the computationally expensive convolutions (3×3 and 5×5).\n","\n","The InceptionNet/GoogleNet architecture consists of 9 inception modules stacked together, with max-pooling layers in between (to halve the spatial dimensions). It consists of 22 layers (27 with the pooling layers). It uses global average pooling after the last inception module."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tII-F338Hj59","executionInfo":{"status":"ok","timestamp":1658111114701,"user_tz":-480,"elapsed":346,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}},"outputId":"67c63425-178d-4430-af1f-5a9269b91ba6"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 128, 128, 128])\n"]}],"source":["import torch\n","import torch.nn as nn\n","\n","class InceptionModule(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(InceptionModule, self).__init__()\n","        relu = nn.ReLU()\n","        self.branch1 = nn.Sequential(\n","                  nn.Conv2d(in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0),\n","                  relu)\n","\n","        conv3_1 = nn.Conv2d(in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n","        conv3_3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.branch2 = nn.Sequential(conv3_1, conv3_3,relu)\n","\n","        conv5_1 = nn.Conv2d(in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n","        conv5_5 = nn.Conv2d(out_channels, out_channels, kernel_size=5, stride=1, padding=2)\n","        self.branch3 = nn.Sequential(conv5_1,conv5_5,relu)\n","\n","        max_pool_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n","        conv_max_1 = nn.Conv2d(in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0)\n","        self.branch4 = nn.Sequential(max_pool_1, conv_max_1,relu)\n","\n","    def forward(self, input):\n","        output1 = self.branch1(input)\n","        output2 = self.branch2(input)\n","        output3 = self.branch3(input)\n","        output4 = self.branch4(input)\n","        return torch.cat([output1, output2, output3, output4], dim=1)\n","\n","model = InceptionModule(in_channels=3,out_channels=32)\n","inp = torch.rand(1,3,128,128)\n","print(model(inp).shape)"]},{"cell_type":"markdown","metadata":{"id":"VWZG6XCBHj5-"},"source":["### Training with alexnet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olu_VzQYHj5_"},"outputs":[],"source":["import os\n","\n","#Numpy is linear algebra lbrary\n","import numpy as np\n","# Matplotlib is a visualizations library \n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":136,"referenced_widgets":["a73f1619c8434b1ba9dc99e1cbcc43d9","4a4d5bedbde14e6ab6fdd57d6be02458","f98eebde71874d5a8202cfbe1ff15419","fcbdf93a27034c25a612ec83f6768280","e0c053a94b7344fd8f843e5d1e0da7b8","27d50cb958ae449f8dece2807f33bbc7","9e8cee9f91dc4c5d8d4ee402c1c31390","f8bd3c47d70143f0a80f2dde9e512c78","78c93f89566e4a58a5cc0e30e78c308d","7ac14fc0108c46f7ab10b324a0ec14eb","8f676b5c341649eb93129297affb8d6f"]},"id":"LCcxBpQAHj6B","executionInfo":{"status":"ok","timestamp":1658111132402,"user_tz":-480,"elapsed":6607,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}},"outputId":"952914a8-3632-4bf5-9e95-181e83fcade5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a73f1619c8434b1ba9dc99e1cbcc43d9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","50000\n","10000\n"]}],"source":["transform = transforms.Compose(\n","    [transforms.Resize(224),\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","batch_size = 4\n","\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","       'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","train_data_size = len(trainloader.dataset)\n","test_data_size = len(testloader.dataset)\n","\n","print(train_data_size)\n","print(test_data_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1YavKV6YHj6C","executionInfo":{"status":"ok","timestamp":1658111136687,"user_tz":-480,"elapsed":1799,"user":{"displayName":"احمد اسعد","userId":"15464471792373836994"}},"outputId":"949618f9-8abf-4226-f33e-5d8f019e5647"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["AlexNet(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n","    (1): ReLU(inplace=True)\n","    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    (4): ReLU(inplace=True)\n","    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (7): ReLU(inplace=True)\n","    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): ReLU(inplace=True)\n","    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n","  (classifier): Sequential(\n","    (0): Dropout(p=0.5, inplace=False)\n","    (1): Linear(in_features=9216, out_features=4096, bias=True)\n","    (2): ReLU(inplace=True)\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(in_features=4096, out_features=4096, bias=True)\n","    (5): ReLU(inplace=True)\n","    (6): Linear(in_features=4096, out_features=10, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":5}],"source":["model = AlexNet(num_classes=10)\n","\n","# 2. LOSS AND OPTIMIZER\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# 3. move the model to GPU\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GjtxnfIQHj6D"},"outputs":[],"source":["import time # to calculate training time\n","\n","def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n","    '''\n","    Function to train and validate\n","    Parameters\n","        :param model: Model to train and validate\n","        :param loss_criterion: Loss Criterion to minimize\n","        :param optimizer: Optimizer for computing gradients\n","        :param epochs: Number of epochs (default=25)\n","  \n","    Returns\n","        model: Trained Model with best validation accuracy\n","        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n","    '''\n","    \n","    start = time.time()\n","    history = []\n","    best_acc = 0.0\n","\n","    for epoch in range(epochs):\n","        epoch_start = time.time()\n","        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n","        \n","        # Set to training mode\n","        model.train()\n","        \n","        # Loss and Accuracy within the epoch\n","        train_loss = 0.0\n","        train_acc = 0.0\n","        \n","        valid_loss = 0.0\n","        valid_acc = 0.0\n","        \n","        for i, (inputs, labels) in enumerate(trainloader):\n","\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            \n","            # Clean existing gradients\n","            optimizer.zero_grad()\n","            \n","            # Forward pass - compute outputs on input data using the model\n","            outputs = model(inputs)\n","            \n","            # Compute loss\n","            loss = loss_criterion(outputs, labels)\n","            \n","            # Backpropagate the gradients\n","            loss.backward()\n","            \n","            # Update the parameters\n","            optimizer.step()\n","            \n","            # Compute the total loss for the batch and add it to train_loss\n","            train_loss += loss.item() * inputs.size(0)\n","            \n","            # Compute the accuracy\n","            ret, predictions = torch.max(outputs.data, 1)\n","            correct_counts = predictions.eq(labels.data.view_as(predictions))\n","            \n","            # Convert correct_counts to float and then compute the mean\n","            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","            \n","            # Compute total accuracy in the whole batch and add to train_acc\n","            train_acc += acc.item() * inputs.size(0)\n","            \n","            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n","\n","            \n","        # Validation - No gradient tracking needed\n","        with torch.no_grad():\n","\n","            # Set to evaluation mode\n","            model.eval()\n","\n","            # Validation loop\n","            for j, (inputs, labels) in enumerate(testloader):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # Forward pass - compute outputs on input data using the model\n","                outputs = model(inputs)\n","\n","                # Compute loss\n","                loss = loss_criterion(outputs, labels)\n","\n","                # Compute the total loss for the batch and add it to valid_loss\n","                valid_loss += loss.item() * inputs.size(0)\n","\n","                # Calculate validation accuracy\n","                ret, predictions = torch.max(outputs.data, 1)\n","                correct_counts = predictions.eq(labels.data.view_as(predictions))\n","\n","                # Convert correct_counts to float and then compute the mean\n","                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","\n","                # Compute total accuracy in the whole batch and add to valid_acc\n","                valid_acc += acc.item() * inputs.size(0)\n","\n","                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n","            \n","        # Find average training loss and training accuracy\n","        avg_train_loss = train_loss/train_data_size \n","        avg_train_acc = train_acc/train_data_size\n","\n","        # Find average training loss and training accuracy\n","        avg_test_loss = valid_loss/test_data_size \n","        avg_test_acc = valid_acc/test_data_size\n","\n","        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n","                \n","        epoch_end = time.time()\n","    \n","        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n","        \n","        # Save if the model has best accuracy till now\n","        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n","            \n","    return model, history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6IGFGAhHj6G","outputId":"7d981d2f-f28e-45f4-e975-2aee7aac963f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1/10\n","Epoch : 000, Training: Loss: 1.7566, Accuracy: 34.2020%, \n","\t\tValidation : Loss : 1.3589, Accuracy: 50.0600%, Time: 301.8623s\n","Epoch: 2/10\n","Epoch : 001, Training: Loss: 1.1327, Accuracy: 59.7100%, \n","\t\tValidation : Loss : 0.8926, Accuracy: 69.0700%, Time: 299.5998s\n","Epoch: 3/10\n","Epoch : 002, Training: Loss: 0.8737, Accuracy: 69.4580%, \n","\t\tValidation : Loss : 0.7548, Accuracy: 73.7100%, Time: 302.3852s\n","Epoch: 4/10\n","Epoch : 003, Training: Loss: 0.7251, Accuracy: 74.6360%, \n","\t\tValidation : Loss : 0.7192, Accuracy: 75.1300%, Time: 304.8721s\n","Epoch: 5/10\n","Epoch : 004, Training: Loss: 0.6158, Accuracy: 78.8000%, \n","\t\tValidation : Loss : 0.6788, Accuracy: 77.2400%, Time: 304.3524s\n","Epoch: 6/10\n","Epoch : 005, Training: Loss: 0.5379, Accuracy: 81.2620%, \n","\t\tValidation : Loss : 0.7010, Accuracy: 77.3800%, Time: 299.8303s\n","Epoch: 7/10\n","Epoch : 006, Training: Loss: 0.4664, Accuracy: 83.8800%, \n","\t\tValidation : Loss : 0.5527, Accuracy: 81.5400%, Time: 293.3633s\n","Epoch: 8/10\n","Epoch : 007, Training: Loss: 0.4127, Accuracy: 85.6600%, \n","\t\tValidation : Loss : 0.5757, Accuracy: 81.5000%, Time: 338.9062s\n","Epoch: 9/10\n","Epoch : 008, Training: Loss: 0.3636, Accuracy: 87.2720%, \n","\t\tValidation : Loss : 0.5854, Accuracy: 81.0500%, Time: 301.6507s\n","Epoch: 10/10\n","Epoch : 009, Training: Loss: 0.3234, Accuracy: 88.7100%, \n","\t\tValidation : Loss : 0.5584, Accuracy: 81.3500%, Time: 344.3763s\n"]}],"source":["# 4. Train the model for 10 epochs\n","\n","num_epochs = 10\n","trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TAc8XnB8Hj6H"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"12. CNN_architectures.ipynb","provenance":[],"collapsed_sections":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"a73f1619c8434b1ba9dc99e1cbcc43d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a4d5bedbde14e6ab6fdd57d6be02458","IPY_MODEL_f98eebde71874d5a8202cfbe1ff15419","IPY_MODEL_fcbdf93a27034c25a612ec83f6768280"],"layout":"IPY_MODEL_e0c053a94b7344fd8f843e5d1e0da7b8"}},"4a4d5bedbde14e6ab6fdd57d6be02458":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_27d50cb958ae449f8dece2807f33bbc7","placeholder":"​","style":"IPY_MODEL_9e8cee9f91dc4c5d8d4ee402c1c31390","value":"100%"}},"f98eebde71874d5a8202cfbe1ff15419":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8bd3c47d70143f0a80f2dde9e512c78","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78c93f89566e4a58a5cc0e30e78c308d","value":170498071}},"fcbdf93a27034c25a612ec83f6768280":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ac14fc0108c46f7ab10b324a0ec14eb","placeholder":"​","style":"IPY_MODEL_8f676b5c341649eb93129297affb8d6f","value":" 170498071/170498071 [00:02&lt;00:00, 89732307.21it/s]"}},"e0c053a94b7344fd8f843e5d1e0da7b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27d50cb958ae449f8dece2807f33bbc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e8cee9f91dc4c5d8d4ee402c1c31390":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8bd3c47d70143f0a80f2dde9e512c78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78c93f89566e4a58a5cc0e30e78c308d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7ac14fc0108c46f7ab10b324a0ec14eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f676b5c341649eb93129297affb8d6f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}